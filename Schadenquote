# 4) KPIs globaux (2020â€“2024)
# â†’ Abschnitt zur Berechnung von globalen (aggregierten) Kennzahlen fÃ¼r die Jahre 2020â€“2024

ep_net <- sum(exposures$earned_premium_net, na.rm = TRUE)
# â†’ Berechnet die gesamte verdiente NettoprÃ¤mie (earned premium net) Ã¼ber alle VertrÃ¤ge.
#   Die Funktion sum() summiert die Spalte 'earned_premium_net' aus dem Dataframe 'exposures'.
#   Argument na.rm = TRUE sorgt dafÃ¼r, dass fehlende Werte (NA) ignoriert werden.

incurred <- sum(claims$paid, na.rm = TRUE) + sum(claims$reserve, na.rm = TRUE)
# â†’ Berechnet die gesamten angefallenen SchÃ¤den (incurred losses):
#   Summe aus bereits gezahlten SchÃ¤den ('paid') und den gebildeten Schadenreserven ('reserve').
#   Beide Summen stammen aus dem Dataframe 'claims'; NA-Werte werden ignoriert.

lr <- ifelse(ep_net > 0, incurred / ep_net, NA_real_)
# â†’ Berechnet die Loss Ratio (Schadenquote) = incurred losses / earned premium net.
#   ifelse() stellt sicher, dass nur berechnet wird, wenn die PrÃ¤mie positiv ist.
#   Falls ep_net â‰¤ 0 oder fehlend, wird stattdessen NA (fehlender Wert) zurÃ¼ckgegeben.
#   NA_real_ spezifiziert, dass das Ergebnis vom Typ 'numeric (real)' ist.



---------------------------------------------------------------------------------------------------------
############################# Explorative Dataanalyse ###########################################

Was diese Analyse zeigt

â¡ï¸ Ziel: Erkennen, welche Gefahrenarten den grÃ¶ÃŸten Beitrag zum Gesamtaufwand leisten.

Typische Erkenntnisse:

â€Fire_Claimâ€œ hat z. B. den hÃ¶chsten Total Incurred â†’ Feuer ist der teuerste Schadenbereich.

â€HSS_Claimâ€œ kÃ¶nnte hÃ¤ufig, aber geringer ausfallen.

â€TPL_Claimâ€œ (Haftpflicht) vielleicht seltener, aber sehr hohe EinzelschÃ¤den.

ğŸ§® In aktuarieller Sprache

Diese Analyse beantwortet:

â€Wie verteilt sich der gesamte Schadenaufwand (Summe aller gezahlten SchÃ¤den) nach Gefahrentyp im Multi-Risk-Portfolio?â€œ

Sie dient zur:

Priorisierung von Risikoursachen (Schwerpunktgefahren),

Produkt- und Tarifsteuerung (wo mÃ¼ssen ZuschlÃ¤ge / PrÃ¤ventionsmaÃŸnahmen hin?),

Risikoberichterstattung (z. B. im internen Controlling oder SFCR-Abschnitt â€Underwriting Performanceâ€œ).

ğŸ’¬ Kurz gesagt

Die Analyse stellt die Gesamtbelastung pro Gefahrenart (Peril) grafisch dar und zeigt, welche Schadenursachen im Multi-Risk-Portfolio finanziell am bedeutendsten sind.

Wenn du mÃ¶chtest, kann ich dir direkt im Anschluss eine zweite Grafik ergÃ¤nzen, die zusÃ¤tzlich die SchadenhÃ¤ufigkeit je Peril (also Anzahl der SchÃ¤den, nicht nur Summe) zeigt â€“ 
das ergibt zusammen eine Severity-vs-Frequency-Analyse, ideal fÃ¼r aktuarielles Reporting. Soll ich das hinzufÃ¼gen?




################################################################################################
## 3) Tarifentwicklung & AuskÃ¶mmlichkeit (Tweedie GLM nur auf Multi-Daten)
################################################################################################


Hier ist der Zweck jedes Schritts â€“ was passiert, warum wirâ€™s tun, und worauf man achten sollte:
3a) Feature-Engineering (nur Multi)


*incurred = Summe aller _Claim-Spalten
Was: Aggregiert alle einzelnen Schadenarten zu einer reinen SchadenhÃ¶he pro Vertrag/Zeile.
Warum: Wir brauchen eine Zielvariable (Response) fÃ¼r Modellierung und Auswertung. Die Summe reduziert Rauschen einzelner Komponenten und entspricht der aktuariell relevanten Gesamtschadenlast.


Proxy-Features (baujahr_klasse, region)
Was: Wir mappen vorhandene Multi-Variablen (z. B. Damage_Revenue â†’ GrÃ¶ÃŸen/Umsatzklasse; Damage_Sites â†’ Anzahl/Standorte).
Warum: Oft fehlen â€perfekteâ€œ Merkmale. Proxys erfassen das systematische Risiko (grÃ¶ÃŸere UmsÃ¤tze/mehr Standorte â‡’ potentiell hÃ¶here Exponierung/SchadenhÃ¤ufigkeit/-hÃ¶he).


earned_exposure = 1
Was: Setzt jede Beobachtung auf eine Einheit-Exponierung.
Warum: Erlaubt gewichtete Modelle/Auswertungen (spÃ¤ter im GLM als weights). Falls echte Exponierung (z. B. Vertragsmonate) vorhanden ist, sollte sie hier verwendet werden.


risk_score & earned_premium_net
Was: Ein einfacher risikoindizierter Score aus Revenue und Sites; daraus eine grobe Netto-VerdienstprÃ¤mie mit MindestprÃ¤mie (pmax).
Warum:


Dient als erklÃ¤rende Variable (Premium-Niveau als Treiber der erwarteten SchadenhÃ¶he).


Hilft, Burning-Cost und Loss Ratio spÃ¤ter in Relation zur PrÃ¤mie zu beurteilen.
Achtung: Der Score ist heuristisch; in Produktion wÃ¼rde man Kalibrierung/Regularisierung und Outlier-Handling prÃ¼fen.




3b) Burning-Cost-Analyse (Orientierung)


Aggregationen nach baujahr_klasse
Was: Summiert Exponierung, PrÃ¤mien und SchÃ¤den je Klasse; berechnet Pure Premium (pure = inc/exp) und Loss Ratio (lr = inc/ep).
Warum:


Baseline gegen die wir Modellresultate spiegeln.


Zeigt Unter-/Ãœberpreisung je Segment (LR > 1 kritisch).


Liefert Hypothesen: Wo weichen Beobachtungen stark ab? Brauchen wir feinere Segmentierung?
Achtung: Kleine Zellen â‡’ hohe Varianz; ggf. Klassen zusammenlegen oder Hierarchie/Empirical-Bayes nutzen.




3c) Tweedie-GLM: reine SchadenhÃ¶he modellieren


Formel (incurred ~ factor(baujahr_klasse) + factor(region) + earned_premium_net)
Was: ErklÃ¤rt die SchadenhÃ¶he durch Strukturmerkmale (Klasse/Region) und das PrÃ¤mienniveau.
Warum: Erfasst sowohl RisikoheterogenitÃ¤t (Faktoren) als auch den Rating-Effekt (Premium als Proxy fÃ¼r Risikoselektion/Deckungsumfang).


Familie: Tweedie (pâ‰ˆ1.5, log-Link)
Was: Verteilungsfamilie zwischen Poisson (p=1) und Gamma (p=2), passend fÃ¼r semikontinuierliche SchadenhÃ¶hen mit vielen Nullen und rechts-schiefen Positivwerten. Log-Link modelliert MultiplikativitÃ¤t.
Warum: Standard im Non-Life-Pricing fÃ¼r Aggregat-Schaden (FrequencyÃ—Severity in einem Schritt).
Achtung: p=1.5 ist Startwertâ€”Profilierung/GRID oder tweedie.profile() sinnvoll; Link=log stabilisiert Varianz, erfordert Kein negatives incurred.


Gewichte = earned_exposure
Was/Warum: Skaliert den Beitrag jeder Beobachtung gemÃ¤ÃŸ Exponierung (hier 1). Bei echten Vertragsmonaten gibtâ€™s konsistente SchÃ¤tzer.


summary(mdl)
Was: Koeffizienten, Signifikanzen, Devianzâ€”ModellgÃ¼te und Treiber erkennen.
Warum: Identifiziert signifikante Segmente und prÃ¼ft, ob Premium-Niveau plausibel wirkt (z. B. hÃ¶here Premium â†’ hÃ¶here erwartete SchadenhÃ¶he kann Selektion/Deckungsbreite reflektieren).


3d) Visualisierung der modellierten Schaden


Histogramm der Fitted Values
Was: Verteilung der vorhergesagten reinen SchadenhÃ¶he.
Warum:


PrÃ¼fen auf Extremwerte/Skewness.


Erkennen von Segmenten mit hoher erwarteter Last (fÃ¼r Pricing-ZuschlÃ¤ge, Underwriting-Regeln).


Erste PlausibilitÃ¤tskontrolle: Stimmen GrÃ¶ÃŸenordnung und Streuung mit Burning-Cost-Eindruck Ã¼berein?
Achtung: ErgÃ¤nze idealerweise Calibration-Plots, Lift/Decile-Analysen, LR vs. Pred-Bins und Out-of-Sample-Checks.





Gute Praxis / Checks (kurz)


Leakage vermeiden: Keine direkt aus incurred abgeleiteten PrÃ¤diktoren ins Modell.


Skalierung/Outlier: Log-Transformation/Robust Scaler fÃ¼r stark schiefe Inputs prÃ¼fen.


Interaktionen: z. B. factor(region):factor(baujahr_klasse) testen, aber Regularisierung (Ridge/Lasso) erwÃ¤gen.


Alternativen: Zweistufig (Poisson-Freq + Gamma-Sev), Zero-Inflated Modelle, oder GBMs als Benchmark.


Bewertung: Devianz-Reduktion, AIC/BIC, Kreuzvalidierung, Backtesting auf Perioden/Regionen.


So fÃ¼gt sich alles zusammen: Feature-Engineering liefert sinnvolle Variablen â†’ Burning-Cost gibt eine robuste, 
modellfreie Referenz â†’ Tweedie-GLM quantifiziert Treiber & Segment-Effekte â†’ Visualisierung prÃ¼ft PlausibilitÃ¤t und unterstÃ¼tzt Preis-/Portfoliosteuerung.

-------------------------------------------------------------------------------------------------------


################################## wie weiÃŸt du das es gaussian ist? #############################################

Super prÃ¤zise Frage ğŸ‘ â€” das zeigt, dass du wirklich verstehst, was hinter dem Code passiert.
Lass uns das Schritt fÃ¼r Schritt sauber aufdrÃ¶seln:

ğŸ§  Warum steht im Code family = "gaussian"?
Weil wir in diesem Schritt nicht die echten SchadenhÃ¶hen direkt prÃ¤dizieren, sondern einen Risikofaktor (Score) lernen, also eine kontinuierliche ZielgrÃ¶ÃŸe.
ğŸ‘‰ DafÃ¼r passt die Gaussâ€™sche Familie (Normalverteilung) mit IdentitÃ¤ts-Link am besten.

ğŸ“ˆ Kontext: Was wir hier eigentlich tun
In deinem Workflow gibt es zwei Modellierungsebenen:
EbeneZielModellFamilyâ‘  Score-Generator (Vorstufe)SchÃ¤tze linearen Risikoscore aus Merkmalen (Damage_Revenue, Damage_Sites, â€¦)Regularisierte Regression (glmnet)gaussianâ‘¡ Tarifmodell (aktuarisch)Modelliert die SchadenhÃ¶he (incurred)Tweedie-GLMtweedie

ğŸ” Was bedeutet "gaussian" bei glmnet?
Wenn du glmnet(..., family = "gaussian") schreibst:
yi=XiâŠ¤Î²+Ïµi,Ïµiâˆ¼N(0,Ïƒ2)y_i = X_i^\top \beta + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2)yiâ€‹=XiâŠ¤â€‹Î²+Ïµiâ€‹,Ïµiâ€‹âˆ¼N(0,Ïƒ2)
â†’ Das ist eine klassische lineare Regression, die durch Regularisierung (Lasso, Ridge oder Elastic Net) stabilisiert wird.
In unserem Fall:


y ist log1p(incurred) â€” also der log-transformierte Schaden, um die Schiefe zu dÃ¤mpfen.


Dadurch ist die Annahme einer (annÃ¤hernd) normalen Fehlerverteilung plausibel.


Wir wollen keine Wahrscheinlichkeiten oder Counts, sondern einen kontinuierlichen Score â†’ also Regression statt Klassifikation.



âš™ï¸ Warum nicht "poisson" oder "tweedie"?
Sehr gute Anschlussfrage â€” theoretisch mÃ¶glich, aber nicht optimal in diesem Schritt:
FamilieTypWann sinnvollWarum hier nichtpoissonZÃ¤hlvariablen (Schadenanzahl)HÃ¤ufigkeit, FrequenzmodelleUnsere y ist kontinuierlich (SchadenhÃ¶he)tweedieMischung aus Poisson/GammaAggregierte SchadenhÃ¶he (mit vielen Nullen)FÃ¼r die aktuarische Modellierung danach gedacht, nicht fÃ¼r den Scoregaussianstetige ZielgrÃ¶ÃŸeScore, Residualanalyse, lineare MusterPerfekt fÃ¼r Vorstufe (Score-SchÃ¤tzung)
Kurz gesagt:

Wir nehmen gaussian, weil wir hier keine Verteilung modellieren, sondern eine glatte, lineare Risikostruktur fÃ¼r den Score schÃ¤tzen wollen.


ğŸ“Š Bonus-Tipp
Wenn du merkst, dass die Verteilung deines log1p(incurred) noch stark schief oder heavy-tailed ist, kannst du testen:
hist(log1p(df_multi$incurred), breaks = 40)

Wenn das einigermaÃŸen glockenfÃ¶rmig aussieht â†’ gaussian passt gut.
Wenn nicht, kannst du alternativ:


robuste Regression nutzen (family = "mgaussian" oder robustbase::lmrob), oder


den Score per Rank-Transformation bauen (z. B. quantilweise).



âœ… Fazit


family = "gaussian" â†’ weil y stetig und (nach Log-Transformation) annÃ¤hernd normalverteilt ist.


Ziel hier: linearen Risikoscore schÃ¤tzen, nicht Schadenverteilung modellieren.


Das eigentliche aktuariell relevante Modell (Tweedie) kommt im nÃ¤chsten Schritt (glm(..., family = tweedie(...))).



Wenn du magst, kann ich dir eine kleine grafische PrÃ¼fung (Q-Q-Plot + Dichtevergleich) einbauen, damit du empirisch siehst, dass die log1p(incurred)-Verteilung die â€gaussianâ€œ-Annahme rechtfertigt.
MÃ¶chtest du das?
--------------------------------------------------------------------------------------

#############################################################################################
################################### Was ist X genau? ########################################

â¡ï¸ X ist die sogenannte Designmatrix (oder Feature-Matrix) â€” also die numerische Darstellung aller erklÃ¤renden Variablen, mit denen das Modell (hier glmnet) arbeitet.
In der Praxis:


Jede Zeile in X = ein Vertrag, Risiko oder Beobachtung.


Jede Spalte in X = ein erklÃ¤rendes Merkmal (â€Featureâ€œ), z. B. Umsatzklasse, Standortanzahl, Region usw.



ğŸ§® Was enthÃ¤lt X hier konkret?
In deinem Beispiel:
X <- model.matrix(
  ~ scale(as.numeric(Damage_Revenue)) + scale(as.numeric(Damage_Sites)),
  data = df_tr
)[, -1]

bedeutet:
Spalte in XBedeutungscale(Damage_Revenue)standardisierter Umsatz (Mittelwert = 0, SD = 1) â€“ Proxy fÃ¼r GrÃ¶ÃŸenklassescale(Damage_Sites)standardisierte Standortanzahl â€“ Proxy fÃ¼r Exponierung / Risikoausdehnung
Das [, -1] entfernt die Intercept-Spalte, weil glmnet automatisch einen eigenen Achsenabschnitt (Î²â‚€) hinzufÃ¼gt.

âš™ï¸ Warum brauchen wir model.matrix() Ã¼berhaupt?
glmnet erwartet eine numerische Matrix X (nicht direkt ein Data Frame oder Formel).
Diese Matrix:


konvertiert kategoriale Variablen automatisch in Dummy-Codierungen (z. B. region_Nord, region_SÃ¼d, â€¦),


entfernt nicht-numerische Spalten (Texte, IDs),


standardisiert die Daten optional,


und liefert ein kompaktes, mathematisch sauberes Format fÃ¼r Regularisierung.



ğŸ’¡ Beispiel zur Veranschaulichung
Angenommen, du hast:
Damage_RevenueDamage_Sites100 0002200 0005150 0003
Dann ergibt:
X <- model.matrix(~ scale(as.numeric(Damage_Revenue)) + scale(as.numeric(Damage_Sites)), data=df_tr)[, -1]

eine Matrix (hier vereinfacht dargestellt):
scaled_Revenuescaled_Sites-1.0-1.21.11.1-0.10.1
Diese Werte werden dann von glmnet genutzt, um die optimalen Gewichte (Î²â‚, Î²â‚‚) zu schÃ¤tzen â€” also wie stark Umsatz und Standorte die SchadenhÃ¶he beeinflussen.

ğŸ“¦ Fazit:

ğŸ”¹ X = numerische Feature-Matrix
ğŸ”¹ Zeilen = Beobachtungen, Spalten = erklÃ¤rende Variablen
ğŸ”¹ erzeugt mit model.matrix(), da glmnet nur numerische Inputs akzeptiert
ğŸ”¹ dient als Input fÃ¼r das regularisierte Modell (glmnet(X, y, â€¦))


MÃ¶chtest du, dass ich dir zeige, wie du zusÃ¤tzlich kategoriale Variablen (z. B. region) korrekt in X einbaust, 
sodass sie automatisch als Dummy-Variablen in die Regularisierung eingehen?

--------------------------------------------------------------------------------------------------
##################################################################################################
####################### Transformation zu multiplikativen Risikoscores ###########################

Exponentiert die standardisierten linearen Scores â†’ positive Risikomultiplikatoren (â‰¥ 0).

Beispiel:

Score = 0 â†’ exp(0) = 1 â†’ â€durchschnittliches Risikoâ€œ

Score = 0.5 â†’ exp(0.5) â‰ˆ 1.65 â†’ 65 % hÃ¶heres Risiko

Score = â€“0.5 â†’ exp(â€“0.5) â‰ˆ 0.61 â†’ 39 % geringeres Risiko

Das ist intuitiv interpretierbar im Pricing-Kontext.

--------------------------------------------------------------------------------------------------
##################################################################################################
Was bedeutet â€Burning Costâ€œ?

Definition (vereinfacht):

Die Burning Cost ist der durchschnittliche Schadenaufwand pro Einheit der versicherten Exponierung.

Formel:

Burning Cost
=
Summe der Sch
a
Â¨
den
Summe der Exponierung
Burning Cost=
Summe der Exponierung
Summe der Sch
a
Â¨
den
	â€‹


In der Praxis:

Sie misst, wie teuer das Risiko â€brenntâ€œ, also wie viel durchschnittlicher Schaden auf eine Einheit Exponierung (z. B. Vertrag, PrÃ¤mie, Versicherungssumme, Fahrzeug, Quadratmeter etc.) entfÃ¤llt.

Sie dient als Grundlage fÃ¼r PrÃ¤mienkalkulation oder Vergleich zwischen Segmenten.

Beispiel:
Wenn in einer Sparte mit 1000 VertrÃ¤gen (Exposures) insgesamt 500 000 â‚¬ SchÃ¤den anfallen:

Burning Cost=500000/1000 = 500â‚¬ pro Vertrag

â†’ Das ist der reine, unbeeinflusste Schadenerwartungswert.

âš–ï¸ Ausrichtung in deinem Code

Hier berechnest du die Burning-Cost-Analyse auf den Testdaten (df_te) â€“ also out-of-sample.
Dadurch kannst du prÃ¼fen, ob dein Modell (PrÃ¤mienniveau) AuskÃ¶mmlichkeit zeigt, d. h. ob Schaden und PrÃ¤mie zueinander passen.