# 4) KPIs globaux (2020–2024)
# → Abschnitt zur Berechnung von globalen (aggregierten) Kennzahlen für die Jahre 2020–2024

ep_net <- sum(exposures$earned_premium_net, na.rm = TRUE)
# → Berechnet die gesamte verdiente Nettoprämie (earned premium net) über alle Verträge.
#   Die Funktion sum() summiert die Spalte 'earned_premium_net' aus dem Dataframe 'exposures'.
#   Argument na.rm = TRUE sorgt dafür, dass fehlende Werte (NA) ignoriert werden.

incurred <- sum(claims$paid, na.rm = TRUE) + sum(claims$reserve, na.rm = TRUE)
# → Berechnet die gesamten angefallenen Schäden (incurred losses):
#   Summe aus bereits gezahlten Schäden ('paid') und den gebildeten Schadenreserven ('reserve').
#   Beide Summen stammen aus dem Dataframe 'claims'; NA-Werte werden ignoriert.

lr <- ifelse(ep_net > 0, incurred / ep_net, NA_real_)
# → Berechnet die Loss Ratio (Schadenquote) = incurred losses / earned premium net.
#   ifelse() stellt sicher, dass nur berechnet wird, wenn die Prämie positiv ist.
#   Falls ep_net ≤ 0 oder fehlend, wird stattdessen NA (fehlender Wert) zurückgegeben.
#   NA_real_ spezifiziert, dass das Ergebnis vom Typ 'numeric (real)' ist.



---------------------------------------------------------------------------------------------------------
############################# Explorative Dataanalyse ###########################################

Was diese Analyse zeigt

➡️ Ziel: Erkennen, welche Gefahrenarten den größten Beitrag zum Gesamtaufwand leisten.

Typische Erkenntnisse:

„Fire_Claim“ hat z. B. den höchsten Total Incurred → Feuer ist der teuerste Schadenbereich.

„HSS_Claim“ könnte häufig, aber geringer ausfallen.

„TPL_Claim“ (Haftpflicht) vielleicht seltener, aber sehr hohe Einzelschäden.

🧮 In aktuarieller Sprache

Diese Analyse beantwortet:

„Wie verteilt sich der gesamte Schadenaufwand (Summe aller gezahlten Schäden) nach Gefahrentyp im Multi-Risk-Portfolio?“

Sie dient zur:

Priorisierung von Risikoursachen (Schwerpunktgefahren),

Produkt- und Tarifsteuerung (wo müssen Zuschläge / Präventionsmaßnahmen hin?),

Risikoberichterstattung (z. B. im internen Controlling oder SFCR-Abschnitt „Underwriting Performance“).

💬 Kurz gesagt

Die Analyse stellt die Gesamtbelastung pro Gefahrenart (Peril) grafisch dar und zeigt, welche Schadenursachen im Multi-Risk-Portfolio finanziell am bedeutendsten sind.

Wenn du möchtest, kann ich dir direkt im Anschluss eine zweite Grafik ergänzen, die zusätzlich die Schadenhäufigkeit je Peril (also Anzahl der Schäden, nicht nur Summe) zeigt – 
das ergibt zusammen eine Severity-vs-Frequency-Analyse, ideal für aktuarielles Reporting. Soll ich das hinzufügen?




################################################################################################
## 3) Tarifentwicklung & Auskömmlichkeit (Tweedie GLM nur auf Multi-Daten)
################################################################################################


Hier ist der Zweck jedes Schritts – was passiert, warum wir’s tun, und worauf man achten sollte:
3a) Feature-Engineering (nur Multi)


*incurred = Summe aller _Claim-Spalten
Was: Aggregiert alle einzelnen Schadenarten zu einer reinen Schadenhöhe pro Vertrag/Zeile.
Warum: Wir brauchen eine Zielvariable (Response) für Modellierung und Auswertung. Die Summe reduziert Rauschen einzelner Komponenten und entspricht der aktuariell relevanten Gesamtschadenlast.


Proxy-Features (baujahr_klasse, region)
Was: Wir mappen vorhandene Multi-Variablen (z. B. Damage_Revenue → Größen/Umsatzklasse; Damage_Sites → Anzahl/Standorte).
Warum: Oft fehlen „perfekte“ Merkmale. Proxys erfassen das systematische Risiko (größere Umsätze/mehr Standorte ⇒ potentiell höhere Exponierung/Schadenhäufigkeit/-höhe).


earned_exposure = 1
Was: Setzt jede Beobachtung auf eine Einheit-Exponierung.
Warum: Erlaubt gewichtete Modelle/Auswertungen (später im GLM als weights). Falls echte Exponierung (z. B. Vertragsmonate) vorhanden ist, sollte sie hier verwendet werden.


risk_score & earned_premium_net
Was: Ein einfacher risikoindizierter Score aus Revenue und Sites; daraus eine grobe Netto-Verdienstprämie mit Mindestprämie (pmax).
Warum:


Dient als erklärende Variable (Premium-Niveau als Treiber der erwarteten Schadenhöhe).


Hilft, Burning-Cost und Loss Ratio später in Relation zur Prämie zu beurteilen.
Achtung: Der Score ist heuristisch; in Produktion würde man Kalibrierung/Regularisierung und Outlier-Handling prüfen.




3b) Burning-Cost-Analyse (Orientierung)


Aggregationen nach baujahr_klasse
Was: Summiert Exponierung, Prämien und Schäden je Klasse; berechnet Pure Premium (pure = inc/exp) und Loss Ratio (lr = inc/ep).
Warum:


Baseline gegen die wir Modellresultate spiegeln.


Zeigt Unter-/Überpreisung je Segment (LR > 1 kritisch).


Liefert Hypothesen: Wo weichen Beobachtungen stark ab? Brauchen wir feinere Segmentierung?
Achtung: Kleine Zellen ⇒ hohe Varianz; ggf. Klassen zusammenlegen oder Hierarchie/Empirical-Bayes nutzen.




3c) Tweedie-GLM: reine Schadenhöhe modellieren


Formel (incurred ~ factor(baujahr_klasse) + factor(region) + earned_premium_net)
Was: Erklärt die Schadenhöhe durch Strukturmerkmale (Klasse/Region) und das Prämienniveau.
Warum: Erfasst sowohl Risikoheterogenität (Faktoren) als auch den Rating-Effekt (Premium als Proxy für Risikoselektion/Deckungsumfang).


Familie: Tweedie (p≈1.5, log-Link)
Was: Verteilungsfamilie zwischen Poisson (p=1) und Gamma (p=2), passend für semikontinuierliche Schadenhöhen mit vielen Nullen und rechts-schiefen Positivwerten. Log-Link modelliert Multiplikativität.
Warum: Standard im Non-Life-Pricing für Aggregat-Schaden (Frequency×Severity in einem Schritt).
Achtung: p=1.5 ist Startwert—Profilierung/GRID oder tweedie.profile() sinnvoll; Link=log stabilisiert Varianz, erfordert Kein negatives incurred.


Gewichte = earned_exposure
Was/Warum: Skaliert den Beitrag jeder Beobachtung gemäß Exponierung (hier 1). Bei echten Vertragsmonaten gibt’s konsistente Schätzer.


summary(mdl)
Was: Koeffizienten, Signifikanzen, Devianz—Modellgüte und Treiber erkennen.
Warum: Identifiziert signifikante Segmente und prüft, ob Premium-Niveau plausibel wirkt (z. B. höhere Premium → höhere erwartete Schadenhöhe kann Selektion/Deckungsbreite reflektieren).


3d) Visualisierung der modellierten Schaden


Histogramm der Fitted Values
Was: Verteilung der vorhergesagten reinen Schadenhöhe.
Warum:


Prüfen auf Extremwerte/Skewness.


Erkennen von Segmenten mit hoher erwarteter Last (für Pricing-Zuschläge, Underwriting-Regeln).


Erste Plausibilitätskontrolle: Stimmen Größenordnung und Streuung mit Burning-Cost-Eindruck überein?
Achtung: Ergänze idealerweise Calibration-Plots, Lift/Decile-Analysen, LR vs. Pred-Bins und Out-of-Sample-Checks.





Gute Praxis / Checks (kurz)


Leakage vermeiden: Keine direkt aus incurred abgeleiteten Prädiktoren ins Modell.


Skalierung/Outlier: Log-Transformation/Robust Scaler für stark schiefe Inputs prüfen.


Interaktionen: z. B. factor(region):factor(baujahr_klasse) testen, aber Regularisierung (Ridge/Lasso) erwägen.


Alternativen: Zweistufig (Poisson-Freq + Gamma-Sev), Zero-Inflated Modelle, oder GBMs als Benchmark.


Bewertung: Devianz-Reduktion, AIC/BIC, Kreuzvalidierung, Backtesting auf Perioden/Regionen.


So fügt sich alles zusammen: Feature-Engineering liefert sinnvolle Variablen → Burning-Cost gibt eine robuste, 
modellfreie Referenz → Tweedie-GLM quantifiziert Treiber & Segment-Effekte → Visualisierung prüft Plausibilität und unterstützt Preis-/Portfoliosteuerung.

-------------------------------------------------------------------------------------------------------


################################## wie weißt du das es gaussian ist? #############################################

Super präzise Frage 👏 — das zeigt, dass du wirklich verstehst, was hinter dem Code passiert.
Lass uns das Schritt für Schritt sauber aufdröseln:

🧠 Warum steht im Code family = "gaussian"?
Weil wir in diesem Schritt nicht die echten Schadenhöhen direkt prädizieren, sondern einen Risikofaktor (Score) lernen, also eine kontinuierliche Zielgröße.
👉 Dafür passt die Gauss’sche Familie (Normalverteilung) mit Identitäts-Link am besten.

📈 Kontext: Was wir hier eigentlich tun
In deinem Workflow gibt es zwei Modellierungsebenen:
EbeneZielModellFamily① Score-Generator (Vorstufe)Schätze linearen Risikoscore aus Merkmalen (Damage_Revenue, Damage_Sites, …)Regularisierte Regression (glmnet)gaussian② Tarifmodell (aktuarisch)Modelliert die Schadenhöhe (incurred)Tweedie-GLMtweedie

🔍 Was bedeutet "gaussian" bei glmnet?
Wenn du glmnet(..., family = "gaussian") schreibst:
yi=Xi⊤β+ϵi,ϵi∼N(0,σ2)y_i = X_i^\top \beta + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2)yi​=Xi⊤​β+ϵi​,ϵi​∼N(0,σ2)
→ Das ist eine klassische lineare Regression, die durch Regularisierung (Lasso, Ridge oder Elastic Net) stabilisiert wird.
In unserem Fall:


y ist log1p(incurred) — also der log-transformierte Schaden, um die Schiefe zu dämpfen.


Dadurch ist die Annahme einer (annähernd) normalen Fehlerverteilung plausibel.


Wir wollen keine Wahrscheinlichkeiten oder Counts, sondern einen kontinuierlichen Score → also Regression statt Klassifikation.



⚙️ Warum nicht "poisson" oder "tweedie"?
Sehr gute Anschlussfrage — theoretisch möglich, aber nicht optimal in diesem Schritt:
FamilieTypWann sinnvollWarum hier nichtpoissonZählvariablen (Schadenanzahl)Häufigkeit, FrequenzmodelleUnsere y ist kontinuierlich (Schadenhöhe)tweedieMischung aus Poisson/GammaAggregierte Schadenhöhe (mit vielen Nullen)Für die aktuarische Modellierung danach gedacht, nicht für den Scoregaussianstetige ZielgrößeScore, Residualanalyse, lineare MusterPerfekt für Vorstufe (Score-Schätzung)
Kurz gesagt:

Wir nehmen gaussian, weil wir hier keine Verteilung modellieren, sondern eine glatte, lineare Risikostruktur für den Score schätzen wollen.


📊 Bonus-Tipp
Wenn du merkst, dass die Verteilung deines log1p(incurred) noch stark schief oder heavy-tailed ist, kannst du testen:
hist(log1p(df_multi$incurred), breaks = 40)

Wenn das einigermaßen glockenförmig aussieht → gaussian passt gut.
Wenn nicht, kannst du alternativ:


robuste Regression nutzen (family = "mgaussian" oder robustbase::lmrob), oder


den Score per Rank-Transformation bauen (z. B. quantilweise).



✅ Fazit


family = "gaussian" → weil y stetig und (nach Log-Transformation) annähernd normalverteilt ist.


Ziel hier: linearen Risikoscore schätzen, nicht Schadenverteilung modellieren.


Das eigentliche aktuariell relevante Modell (Tweedie) kommt im nächsten Schritt (glm(..., family = tweedie(...))).



Wenn du magst, kann ich dir eine kleine grafische Prüfung (Q-Q-Plot + Dichtevergleich) einbauen, damit du empirisch siehst, dass die log1p(incurred)-Verteilung die „gaussian“-Annahme rechtfertigt.
Möchtest du das?
--------------------------------------------------------------------------------------

#############################################################################################
################################### Was ist X genau? ########################################

➡️ X ist die sogenannte Designmatrix (oder Feature-Matrix) — also die numerische Darstellung aller erklärenden Variablen, mit denen das Modell (hier glmnet) arbeitet.
In der Praxis:


Jede Zeile in X = ein Vertrag, Risiko oder Beobachtung.


Jede Spalte in X = ein erklärendes Merkmal („Feature“), z. B. Umsatzklasse, Standortanzahl, Region usw.



🧮 Was enthält X hier konkret?
In deinem Beispiel:
X <- model.matrix(
  ~ scale(as.numeric(Damage_Revenue)) + scale(as.numeric(Damage_Sites)),
  data = df_tr
)[, -1]

bedeutet:
Spalte in XBedeutungscale(Damage_Revenue)standardisierter Umsatz (Mittelwert = 0, SD = 1) – Proxy für Größenklassescale(Damage_Sites)standardisierte Standortanzahl – Proxy für Exponierung / Risikoausdehnung
Das [, -1] entfernt die Intercept-Spalte, weil glmnet automatisch einen eigenen Achsenabschnitt (β₀) hinzufügt.

⚙️ Warum brauchen wir model.matrix() überhaupt?
glmnet erwartet eine numerische Matrix X (nicht direkt ein Data Frame oder Formel).
Diese Matrix:


konvertiert kategoriale Variablen automatisch in Dummy-Codierungen (z. B. region_Nord, region_Süd, …),


entfernt nicht-numerische Spalten (Texte, IDs),


standardisiert die Daten optional,


und liefert ein kompaktes, mathematisch sauberes Format für Regularisierung.



💡 Beispiel zur Veranschaulichung
Angenommen, du hast:
Damage_RevenueDamage_Sites100 0002200 0005150 0003
Dann ergibt:
X <- model.matrix(~ scale(as.numeric(Damage_Revenue)) + scale(as.numeric(Damage_Sites)), data=df_tr)[, -1]

eine Matrix (hier vereinfacht dargestellt):
scaled_Revenuescaled_Sites-1.0-1.21.11.1-0.10.1
Diese Werte werden dann von glmnet genutzt, um die optimalen Gewichte (β₁, β₂) zu schätzen — also wie stark Umsatz und Standorte die Schadenhöhe beeinflussen.

📦 Fazit:

🔹 X = numerische Feature-Matrix
🔹 Zeilen = Beobachtungen, Spalten = erklärende Variablen
🔹 erzeugt mit model.matrix(), da glmnet nur numerische Inputs akzeptiert
🔹 dient als Input für das regularisierte Modell (glmnet(X, y, …))


Möchtest du, dass ich dir zeige, wie du zusätzlich kategoriale Variablen (z. B. region) korrekt in X einbaust, 
sodass sie automatisch als Dummy-Variablen in die Regularisierung eingehen?

--------------------------------------------------------------------------------------------------
##################################################################################################
####################### Transformation zu multiplikativen Risikoscores ###########################

Exponentiert die standardisierten linearen Scores → positive Risikomultiplikatoren (≥ 0).

Beispiel:

Score = 0 → exp(0) = 1 → „durchschnittliches Risiko“

Score = 0.5 → exp(0.5) ≈ 1.65 → 65 % höheres Risiko

Score = –0.5 → exp(–0.5) ≈ 0.61 → 39 % geringeres Risiko

Das ist intuitiv interpretierbar im Pricing-Kontext.

--------------------------------------------------------------------------------------------------
##################################################################################################
Was bedeutet „Burning Cost“?

Definition (vereinfacht):

Die Burning Cost ist der durchschnittliche Schadenaufwand pro Einheit der versicherten Exponierung.

Formel:

Burning Cost
=
Summe der Sch
a
¨
den
Summe der Exponierung
Burning Cost=
Summe der Exponierung
Summe der Sch
a
¨
den
	​


In der Praxis:

Sie misst, wie teuer das Risiko „brennt“, also wie viel durchschnittlicher Schaden auf eine Einheit Exponierung (z. B. Vertrag, Prämie, Versicherungssumme, Fahrzeug, Quadratmeter etc.) entfällt.

Sie dient als Grundlage für Prämienkalkulation oder Vergleich zwischen Segmenten.

Beispiel:
Wenn in einer Sparte mit 1000 Verträgen (Exposures) insgesamt 500 000 € Schäden anfallen:

Burning Cost=500000/1000 = 500€ pro Vertrag

→ Das ist der reine, unbeeinflusste Schadenerwartungswert.

⚖️ Ausrichtung in deinem Code

Hier berechnest du die Burning-Cost-Analyse auf den Testdaten (df_te) – also out-of-sample.
Dadurch kannst du prüfen, ob dein Modell (Prämienniveau) Auskömmlichkeit zeigt, d. h. ob Schaden und Prämie zueinander passen.